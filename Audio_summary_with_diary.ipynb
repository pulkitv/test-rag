{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNBiW/PB3Bz8FB4j0+SCche",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pulkitv/test-rag/blob/main/Audio_summary_with_diary.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai==0.28\n",
        "!pip install pyannote.audio\n",
        "!pip install openai-whisper\n",
        "\n",
        "!pip install torch==1.10.0+cu102\n",
        "!pip install pyannote.audio==0.0.1\n",
        "\n",
        "!pip install git+https://github.com/openai/whisper.git\n",
        "!pip install gtts\n",
        "!pip install pydub"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "kEdn-unu7SWG",
        "outputId": "a3a0ed78-90d2-420b-933d-d7f228a8ca9c",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai==0.28\n",
            "  Downloading openai-0.28.0-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.11/dist-packages (from openai==0.28) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from openai==0.28) (4.67.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from openai==0.28) (3.11.15)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20->openai==0.28) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20->openai==0.28) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20->openai==0.28) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20->openai==0.28) (2025.1.31)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28) (1.20.0)\n",
            "Downloading openai-0.28.0-py3-none-any.whl (76 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/76.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.5/76.5 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: openai\n",
            "  Attempting uninstall: openai\n",
            "    Found existing installation: openai 1.75.0\n",
            "    Uninstalling openai-1.75.0:\n",
            "      Successfully uninstalled openai-1.75.0\n",
            "Successfully installed openai-0.28.0\n",
            "Collecting pyannote.audio\n",
            "  Downloading pyannote.audio-3.3.2-py2.py3-none-any.whl.metadata (11 kB)\n",
            "Collecting asteroid-filterbanks>=0.4 (from pyannote.audio)\n",
            "  Downloading asteroid_filterbanks-0.4.0-py3-none-any.whl.metadata (3.3 kB)\n",
            "Requirement already satisfied: einops>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pyannote.audio) (0.8.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.13.0 in /usr/local/lib/python3.11/dist-packages (from pyannote.audio) (0.30.2)\n",
            "Collecting lightning>=2.0.1 (from pyannote.audio)\n",
            "  Downloading lightning-2.5.1.post0-py3-none-any.whl.metadata (39 kB)\n",
            "Collecting omegaconf<3.0,>=2.1 (from pyannote.audio)\n",
            "  Downloading omegaconf-2.3.0-py3-none-any.whl.metadata (3.9 kB)\n",
            "Collecting pyannote.core>=5.0.0 (from pyannote.audio)\n",
            "  Downloading pyannote.core-5.0.0-py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting pyannote.database>=5.0.1 (from pyannote.audio)\n",
            "  Downloading pyannote.database-5.1.3-py3-none-any.whl.metadata (1.1 kB)\n",
            "Collecting pyannote.metrics>=3.2 (from pyannote.audio)\n",
            "  Downloading pyannote.metrics-3.2.1-py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting pyannote.pipeline>=3.0.1 (from pyannote.audio)\n",
            "  Downloading pyannote.pipeline-3.0.1-py3-none-any.whl.metadata (897 bytes)\n",
            "Collecting pytorch-metric-learning>=2.1.0 (from pyannote.audio)\n",
            "  Downloading pytorch_metric_learning-2.8.1-py3-none-any.whl.metadata (18 kB)\n",
            "Requirement already satisfied: rich>=12.0.0 in /usr/local/lib/python3.11/dist-packages (from pyannote.audio) (13.9.4)\n",
            "Collecting semver>=3.0.0 (from pyannote.audio)\n",
            "  Downloading semver-3.0.4-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.11/dist-packages (from pyannote.audio) (0.13.1)\n",
            "Collecting speechbrain>=1.0.0 (from pyannote.audio)\n",
            "  Downloading speechbrain-1.0.3-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting tensorboardX>=2.6 (from pyannote.audio)\n",
            "  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pyannote.audio) (2.6.0+cu124)\n",
            "Collecting torch-audiomentations>=0.11.0 (from pyannote.audio)\n",
            "  Downloading torch_audiomentations-0.12.0-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: torchaudio>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from pyannote.audio) (2.6.0+cu124)\n",
            "Collecting torchmetrics>=0.11.0 (from pyannote.audio)\n",
            "  Downloading torchmetrics-1.7.1-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from asteroid-filterbanks>=0.4->pyannote.audio) (2.0.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from asteroid-filterbanks>=0.4->pyannote.audio) (4.13.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.13.0->pyannote.audio) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.13.0->pyannote.audio) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.13.0->pyannote.audio) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.13.0->pyannote.audio) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.13.0->pyannote.audio) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.13.0->pyannote.audio) (4.67.1)\n",
            "Collecting lightning-utilities<2.0,>=0.10.0 (from lightning>=2.0.1->pyannote.audio)\n",
            "  Downloading lightning_utilities-0.14.3-py3-none-any.whl.metadata (5.6 kB)\n",
            "Collecting pytorch-lightning (from lightning>=2.0.1->pyannote.audio)\n",
            "  Downloading pytorch_lightning-2.5.1.post0-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting antlr4-python3-runtime==4.9.* (from omegaconf<3.0,>=2.1->pyannote.audio)\n",
            "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: sortedcontainers>=2.0.4 in /usr/local/lib/python3.11/dist-packages (from pyannote.core>=5.0.0->pyannote.audio) (2.4.0)\n",
            "Requirement already satisfied: scipy>=1.1 in /usr/local/lib/python3.11/dist-packages (from pyannote.core>=5.0.0->pyannote.audio) (1.14.1)\n",
            "Requirement already satisfied: pandas>=0.19 in /usr/local/lib/python3.11/dist-packages (from pyannote.database>=5.0.1->pyannote.audio) (2.2.2)\n",
            "Requirement already satisfied: typer>=0.12.1 in /usr/local/lib/python3.11/dist-packages (from pyannote.database>=5.0.1->pyannote.audio) (0.15.2)\n",
            "Requirement already satisfied: scikit-learn>=0.17.1 in /usr/local/lib/python3.11/dist-packages (from pyannote.metrics>=3.2->pyannote.audio) (1.6.1)\n",
            "Collecting docopt>=0.6.2 (from pyannote.metrics>=3.2->pyannote.audio)\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.11/dist-packages (from pyannote.metrics>=3.2->pyannote.audio) (0.9.0)\n",
            "Requirement already satisfied: matplotlib>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pyannote.metrics>=3.2->pyannote.audio) (3.10.0)\n",
            "Requirement already satisfied: sympy>=1.1 in /usr/local/lib/python3.11/dist-packages (from pyannote.metrics>=3.2->pyannote.audio) (1.13.1)\n",
            "Collecting optuna>=3.1 (from pyannote.pipeline>=3.0.1->pyannote.audio)\n",
            "  Downloading optuna-4.3.0-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=12.0.0->pyannote.audio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=12.0.0->pyannote.audio) (2.18.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/dist-packages (from soundfile>=0.12.1->pyannote.audio) (1.17.1)\n",
            "Collecting hyperpyyaml (from speechbrain>=1.0.0->pyannote.audio)\n",
            "  Downloading HyperPyYAML-1.2.2-py3-none-any.whl.metadata (7.6 kB)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from speechbrain>=1.0.0->pyannote.audio) (1.4.2)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from speechbrain>=1.0.0->pyannote.audio) (0.2.0)\n",
            "Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.11/dist-packages (from tensorboardX>=2.6->pyannote.audio) (5.29.4)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->pyannote.audio) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->pyannote.audio) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=2.0.0->pyannote.audio)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=2.0.0->pyannote.audio)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=2.0.0->pyannote.audio)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.0.0->pyannote.audio)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.0.0->pyannote.audio)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.0.0->pyannote.audio)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.0.0->pyannote.audio)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.0.0->pyannote.audio)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.0.0->pyannote.audio)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->pyannote.audio) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->pyannote.audio) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->pyannote.audio) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.0.0->pyannote.audio)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->pyannote.audio) (3.2.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.1->pyannote.metrics>=3.2->pyannote.audio) (1.3.0)\n",
            "Collecting julius<0.3,>=0.2.3 (from torch-audiomentations>=0.11.0->pyannote.audio)\n",
            "  Downloading julius-0.2.7.tar.gz (59 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.6/59.6 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting torch-pitch-shift>=1.2.2 (from torch-audiomentations>=0.11.0->pyannote.audio)\n",
            "  Downloading torch_pitch_shift-1.2.5-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0->soundfile>=0.12.1->pyannote.audio) (2.22)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<2026.0,>=2022.5.0->lightning>=2.0.1->pyannote.audio) (3.11.15)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from lightning-utilities<2.0,>=0.10.0->lightning>=2.0.1->pyannote.audio) (75.2.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=12.0.0->pyannote.audio) (0.1.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio) (2.8.2)\n",
            "Collecting alembic>=1.5.0 (from optuna>=3.1->pyannote.pipeline>=3.0.1->pyannote.audio)\n",
            "  Downloading alembic-1.15.2-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting colorlog (from optuna>=3.1->pyannote.pipeline>=3.0.1->pyannote.audio)\n",
            "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna>=3.1->pyannote.pipeline>=3.0.1->pyannote.audio) (2.0.40)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.19->pyannote.database>=5.0.1->pyannote.audio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.19->pyannote.database>=5.0.1->pyannote.audio) (2025.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.17.1->pyannote.metrics>=3.2->pyannote.audio) (3.6.0)\n",
            "Collecting primePy>=1.3 (from torch-pitch-shift>=1.2.2->torch-audiomentations>=0.11.0->pyannote.audio)\n",
            "  Downloading primePy-1.3-py3-none-any.whl.metadata (4.8 kB)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.12.1->pyannote.database>=5.0.1->pyannote.audio) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.12.1->pyannote.database>=5.0.1->pyannote.audio) (1.5.4)\n",
            "Collecting ruamel.yaml>=0.17.28 (from hyperpyyaml->speechbrain>=1.0.0->pyannote.audio)\n",
            "  Downloading ruamel.yaml-0.18.10-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->pyannote.audio) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.13.0->pyannote.audio) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.13.0->pyannote.audio) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.13.0->pyannote.audio) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.13.0->pyannote.audio) (2025.1.31)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning>=2.0.1->pyannote.audio) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning>=2.0.1->pyannote.audio) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning>=2.0.1->pyannote.audio) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning>=2.0.1->pyannote.audio) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning>=2.0.1->pyannote.audio) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning>=2.0.1->pyannote.audio) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning>=2.0.1->pyannote.audio) (1.20.0)\n",
            "Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic>=1.5.0->optuna>=3.1->pyannote.pipeline>=3.0.1->pyannote.audio) (1.1.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio) (1.17.0)\n",
            "Collecting ruamel.yaml.clib>=0.2.7 (from ruamel.yaml>=0.17.28->hyperpyyaml->speechbrain>=1.0.0->pyannote.audio)\n",
            "  Downloading ruamel.yaml.clib-0.2.12-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna>=3.1->pyannote.pipeline>=3.0.1->pyannote.audio) (3.2.1)\n",
            "Downloading pyannote.audio-3.3.2-py2.py3-none-any.whl (898 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m898.7/898.7 kB\u001b[0m \u001b[31m33.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading asteroid_filterbanks-0.4.0-py3-none-any.whl (29 kB)\n",
            "Downloading lightning-2.5.1.post0-py3-none-any.whl (819 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m819.0/819.0 kB\u001b[0m \u001b[31m49.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyannote.core-5.0.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.5/58.5 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyannote.database-5.1.3-py3-none-any.whl (48 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.1/48.1 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyannote.metrics-3.2.1-py3-none-any.whl (51 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.4/51.4 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyannote.pipeline-3.0.1-py3-none-any.whl (31 kB)\n",
            "Downloading pytorch_metric_learning-2.8.1-py3-none-any.whl (125 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.9/125.9 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading semver-3.0.4-py3-none-any.whl (17 kB)\n",
            "Downloading speechbrain-1.0.3-py3-none-any.whl (864 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m864.1/864.1 kB\u001b[0m \u001b[31m57.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m116.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m94.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m57.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m44.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m104.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torch_audiomentations-0.12.0-py3-none-any.whl (48 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.5/48.5 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchmetrics-1.7.1-py3-none-any.whl (961 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m961.5/961.5 kB\u001b[0m \u001b[31m59.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning_utilities-0.14.3-py3-none-any.whl (28 kB)\n",
            "Downloading optuna-4.3.0-py3-none-any.whl (386 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m386.6/386.6 kB\u001b[0m \u001b[31m37.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torch_pitch_shift-1.2.5-py3-none-any.whl (5.0 kB)\n",
            "Downloading HyperPyYAML-1.2.2-py3-none-any.whl (16 kB)\n",
            "Downloading pytorch_lightning-2.5.1.post0-py3-none-any.whl (823 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.1/823.1 kB\u001b[0m \u001b[31m61.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading alembic-1.15.2-py3-none-any.whl (231 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m231.9/231.9 kB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading primePy-1.3-py3-none-any.whl (4.0 kB)\n",
            "Downloading ruamel.yaml-0.18.10-py3-none-any.whl (117 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.7/117.7 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
            "Downloading ruamel.yaml.clib-0.2.12-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (739 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m739.1/739.1 kB\u001b[0m \u001b[31m51.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: antlr4-python3-runtime, docopt, julius\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144554 sha256=573404a0e5b741de109ab9a72efd96fa3fd12a68f0735a86a62b8175aeb5aa85\n",
            "  Stored in directory: /root/.cache/pip/wheels/1a/97/32/461f837398029ad76911109f07047fde1d7b661a147c7c56d1\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=39afcc7db550f7a76e1fa55ad6d04e759c488cf8f5824c8e75524d2ef323e6a9\n",
            "  Stored in directory: /root/.cache/pip/wheels/1a/b0/8c/4b75c4116c31f83c8f9f047231251e13cc74481cca4a78a9ce\n",
            "  Building wheel for julius (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for julius: filename=julius-0.2.7-py3-none-any.whl size=21870 sha256=35bb2611d3b183092311ecd53e758980ab271e1de61a917ed774a1eb63999ae8\n",
            "  Stored in directory: /root/.cache/pip/wheels/16/15/d4/edd724cefe78050a6ba3344b8b0c6672db829a799dbb9f81ff\n",
            "Successfully built antlr4-python3-runtime docopt julius\n",
            "Installing collected packages: primePy, docopt, antlr4-python3-runtime, tensorboardX, semver, ruamel.yaml.clib, omegaconf, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, lightning-utilities, colorlog, ruamel.yaml, pyannote.core, nvidia-cusparse-cu12, nvidia-cudnn-cu12, alembic, optuna, nvidia-cusolver-cu12, hyperpyyaml, pyannote.database, torchmetrics, pytorch-metric-learning, pyannote.pipeline, pyannote.metrics, julius, asteroid-filterbanks, torch-pitch-shift, speechbrain, pytorch-lightning, torch-audiomentations, lightning, pyannote.audio\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed alembic-1.15.2 antlr4-python3-runtime-4.9.3 asteroid-filterbanks-0.4.0 colorlog-6.9.0 docopt-0.6.2 hyperpyyaml-1.2.2 julius-0.2.7 lightning-2.5.1.post0 lightning-utilities-0.14.3 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 omegaconf-2.3.0 optuna-4.3.0 primePy-1.3 pyannote.audio-3.3.2 pyannote.core-5.0.0 pyannote.database-5.1.3 pyannote.metrics-3.2.1 pyannote.pipeline-3.0.1 pytorch-lightning-2.5.1.post0 pytorch-metric-learning-2.8.1 ruamel.yaml-0.18.10 ruamel.yaml.clib-0.2.12 semver-3.0.4 speechbrain-1.0.3 tensorboardX-2.6.2.2 torch-audiomentations-0.12.0 torch-pitch-shift-1.2.5 torchmetrics-1.7.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pydevd_plugins"
                ]
              },
              "id": "fea407f8ad124ad88ddb7f68854e6581"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai-whisper\n",
            "  Downloading openai-whisper-20240930.tar.gz (800 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/800.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m800.5/800.5 kB\u001b[0m \u001b[31m42.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (0.60.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (2.0.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (2.6.0+cu124)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (4.67.1)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (10.6.0)\n",
            "Collecting tiktoken (from openai-whisper)\n",
            "  Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: triton>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (3.2.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba->openai-whisper) (0.43.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken->openai-whisper) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken->openai-whisper) (2.32.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (12.4.127)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->openai-whisper) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2025.1.31)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->openai-whisper) (3.0.2)\n",
            "Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m74.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: openai-whisper\n",
            "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai-whisper: filename=openai_whisper-20240930-py3-none-any.whl size=803406 sha256=a556e935792a695faa232a5fc1593021f17afcb01a945b6905481bda6623358f\n",
            "  Stored in directory: /root/.cache/pip/wheels/2f/f2/ce/6eb23db4091d026238ce76703bd66da60b969d70bcc81d5d3a\n",
            "Successfully built openai-whisper\n",
            "Installing collected packages: tiktoken, openai-whisper\n",
            "Successfully installed openai-whisper-20240930 tiktoken-0.9.0\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement torch==1.10.0+cu102 (from versions: 1.13.0, 1.13.1, 2.0.0, 2.0.1, 2.1.0, 2.1.1, 2.1.2, 2.2.0, 2.2.1, 2.2.2, 2.3.0, 2.3.1, 2.4.0, 2.4.1, 2.5.0, 2.5.1, 2.6.0, 2.7.0)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for torch==1.10.0+cu102\u001b[0m\u001b[31m\n",
            "\u001b[0mCollecting pyannote.audio==0.0.1\n",
            "  Downloading pyannote.audio-0.0.1-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: asteroid-filterbanks<0.5,>=0.4 in /usr/local/lib/python3.11/dist-packages (from pyannote.audio==0.0.1) (0.4.0)\n",
            "Collecting backports.cached-property (from pyannote.audio==0.0.1)\n",
            "  Downloading backports.cached_property-1.0.2-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting einops<0.4.0,>=0.3 (from pyannote.audio==0.0.1)\n",
            "  Downloading einops-0.3.2-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting hmmlearn<0.3,>=0.2.7 (from pyannote.audio==0.0.1)\n",
            "  Downloading hmmlearn-0.2.8.tar.gz (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.9/61.9 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting huggingface-hub<0.9,>=0.7 (from pyannote.audio==0.0.1)\n",
            "  Downloading huggingface_hub-0.8.1-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting networkx<3.0,>=2.6 (from pyannote.audio==0.0.1)\n",
            "  Downloading networkx-2.8.8-py3-none-any.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: omegaconf<3.0,>=2.1 in /usr/local/lib/python3.11/dist-packages (from pyannote.audio==0.0.1) (2.3.0)\n",
            "Collecting pyannote.core<5.0,>=4.4 (from pyannote.audio==0.0.1)\n",
            "  Downloading pyannote.core-4.5-py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting pyannote.database<5.0,>=4.1.1 (from pyannote.audio==0.0.1)\n",
            "  Downloading pyannote.database-4.1.3-py3-none-any.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: pyannote.metrics<4.0,>=3.2 in /usr/local/lib/python3.11/dist-packages (from pyannote.audio==0.0.1) (3.2.1)\n",
            "Collecting pyannote.pipeline<3.0,>=2.3 (from pyannote.audio==0.0.1)\n",
            "  Downloading pyannote.pipeline-2.3-py3-none-any.whl.metadata (955 bytes)\n",
            "Collecting pytorch-lightning<1.7,>=1.5.4 (from pyannote.audio==0.0.1)\n",
            "  Downloading pytorch_lightning-1.6.5.post0-py3-none-any.whl.metadata (31 kB)\n",
            "Collecting pytorch-metric-learning<2.0,>=1.0.0 (from pyannote.audio==0.0.1)\n",
            "  Downloading pytorch_metric_learning-1.7.3-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting semver<3.0,>=2.10.2 (from pyannote.audio==0.0.1)\n",
            "  Downloading semver-2.13.0-py2.py3-none-any.whl.metadata (5.0 kB)\n",
            "Collecting singledispatchmethod (from pyannote.audio==0.0.1)\n",
            "  Downloading singledispatchmethod-1.0-py2.py3-none-any.whl.metadata (3.6 kB)\n",
            "Collecting soundfile<0.11,>=0.10.2 (from pyannote.audio==0.0.1)\n",
            "  Downloading SoundFile-0.10.3.post1-py2.py3-none-any.whl.metadata (11 kB)\n",
            "Collecting speechbrain<0.6,>=0.5.12 (from pyannote.audio==0.0.1)\n",
            "  Downloading speechbrain-0.5.16-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: torch>=1.9 in /usr/local/lib/python3.11/dist-packages (from pyannote.audio==0.0.1) (2.6.0+cu124)\n",
            "Requirement already satisfied: torch-audiomentations>=0.11.0 in /usr/local/lib/python3.11/dist-packages (from pyannote.audio==0.0.1) (0.12.0)\n",
            "INFO: pip is looking at multiple versions of pyannote-audio to determine which version is compatible with other requirements. This could take a while.\n",
            "\u001b[31mERROR: Ignored the following yanked versions: 2.0.0\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement torchaudio<1.0,>=0.10 (from pyannote-audio) (from versions: 2.0.1, 2.0.2, 2.1.0, 2.1.1, 2.1.2, 2.2.0, 2.2.1, 2.2.2, 2.3.0, 2.3.1, 2.4.0, 2.4.1, 2.5.0, 2.5.1, 2.6.0, 2.7.0)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for torchaudio<1.0,>=0.10\u001b[0m\u001b[31m\n",
            "\u001b[0mCollecting git+https://github.com/openai/whisper.git\n",
            "  Cloning https://github.com/openai/whisper.git to /tmp/pip-req-build-pw18wslw\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/whisper.git /tmp/pip-req-build-pw18wslw\n",
            "  Resolved https://github.com/openai/whisper.git to commit 517a43ecd132a2089d85f4ebc044728a71d49f6e\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20240930) (10.6.0)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20240930) (0.60.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20240930) (2.0.2)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20240930) (0.9.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20240930) (2.6.0+cu124)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20240930) (4.67.1)\n",
            "Requirement already satisfied: triton>=2 in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20240930) (3.2.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba->openai-whisper==20240930) (0.43.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken->openai-whisper==20240930) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken->openai-whisper==20240930) (2.32.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (12.4.127)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->openai-whisper==20240930) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (2025.1.31)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->openai-whisper==20240930) (3.0.2)\n",
            "Collecting gtts\n",
            "  Downloading gTTS-2.5.4-py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.11/dist-packages (from gtts) (2.32.3)\n",
            "Requirement already satisfied: click<8.2,>=7.1 in /usr/local/lib/python3.11/dist-packages (from gtts) (8.1.8)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->gtts) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->gtts) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->gtts) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->gtts) (2025.1.31)\n",
            "Downloading gTTS-2.5.4-py3-none-any.whl (29 kB)\n",
            "Installing collected packages: gtts\n",
            "Successfully installed gtts-2.5.4\n",
            "Collecting pydub\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Installing collected packages: pydub\n",
            "Successfully installed pydub-0.25.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
        "# GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "# GOOGLE_CSE_ID = userdata.get('GOOGLE_CSE_ID')\n",
        "# ACTIVELOOP_TOKEN = userdata.get('ACTIVELOOP_TOKEN')\n",
        "# LANGCHAIN_API_KEY = userdata.get('LANGCHAIN_API_KEY')\n",
        "HUGGINGFACE_API_KEY = userdata.get('HUGGINGFACE_API_KEY')\n",
        "GMAIL_PASSWORD = userdata.get('GMAIL_PASSWORD')\n",
        "\n",
        "\n",
        "# os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY\n",
        "# os.environ[\"GOOGLE_CSE_ID\"] = GOOGLE_CSE_ID\n",
        "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
        "# os.environ[\"ACTIVELOOP_TOKEN\"] = ACTIVELOOP_TOKEN\n",
        "# os.environ[\"LANGCHAIN_API_KEY\"] = LANGCHAIN_API_KEY\n",
        "os.environ[\"HUGGINGFACE_API_KEY\"] = HUGGINGFACE_API_KEY\n",
        "os.environ[\"GMAIL_PASSWORD\"] = GMAIL_PASSWORD\n",
        "\n",
        "print(\"Done\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SvajPgUj3fVl",
        "outputId": "89d3cd7e-609a-4bf9-9867-3cc03ed35747",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "import shutil\n",
        "import torch\n",
        "\n",
        "# Check if GPU is available (only use GPU-related code in specific cells)\n",
        "gpu_available = torch.cuda.is_available()\n",
        "\n",
        "if gpu_available:\n",
        "    print(\"GPU is available.\")\n",
        "else:\n",
        "    print(\"GPU is not available. Using CPU.\")\n",
        "\n",
        "# Only use GPU if it's available\n",
        "if gpu_available:\n",
        "    # Load the Whisper model and move it to GPU\n",
        "    import whisper\n",
        "    model = whisper.load_model(\"base\").to(torch.device(\"cuda\"))\n",
        "    print(\"Model loaded on GPU.\")\n",
        "    # Upload audio file\n",
        "    uploaded = files.upload().to(torch.device(\"cuda\"))\n",
        "\n",
        "else:\n",
        "    # Load the model on CPU if GPU is not available\n",
        "    uploaded = files.upload()\n",
        "    print(\"Model loaded on CPU.\")\n",
        "\n",
        "\n",
        "# Move the model back to CPU after processing\n",
        "if gpu_available:\n",
        "    model.to(torch.device(\"cpu\"))\n",
        "    print(\"Model moved back to CPU to free up GPU resources.\")\n",
        "    torch.cuda.empty_cache()\n",
        "    print(\"GPU memory cleared.\")\n",
        "\n",
        "\n",
        "# Move the uploaded file to the working directory\n",
        "audio_file_path = list(uploaded.keys())[0]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "gr6k0iDd2zII",
        "outputId": "8ca3f233-a8ad-4810-da3b-0c6781c075bf",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-a3a7f180-af40-4f5f-9bcb-1ae05ea13f32\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-a3a7f180-af40-4f5f-9bcb-1ae05ea13f32\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Meeting 1.mp3 to Meeting 1.mp3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import whisper\n",
        "from pyannote.audio.pipelines import SpeakerDiarization\n",
        "from pyannote.core import Segment\n",
        "import torch\n",
        "import warnings\n",
        "import logging\n",
        "\n",
        "# Suppress FP16 warning on CPU\n",
        "warnings.filterwarnings(\"ignore\", message=\"FP16 is not supported on CPU\")\n",
        "\n",
        "# Suppress PyTorch warnings and logs\n",
        "logging.getLogger(\"torch\").setLevel(logging.ERROR)\n",
        "\n",
        "# Suppress SpeechBrain and pyannote logging\n",
        "logging.getLogger(\"speechbrain\").setLevel(logging.ERROR)\n",
        "logging.getLogger(\"pyannote\").setLevel(logging.ERROR)\n",
        "\n",
        "# Load Whisper model for transcription\n",
        "model = whisper.load_model(\"base\")\n",
        "\n",
        "# Perform transcription with Whisper and get word-level timestamps\n",
        "result = model.transcribe(audio_file_path, word_timestamps=True, task='translate')\n",
        "\n",
        "# Get Whisper's word-level segments (start, end, and text)\n",
        "segments = result['segments']  # This contains the start, end, and text of each segment\n",
        "\n",
        "# Load pyannote's pre-trained speaker diarization model\n",
        "pipeline = SpeakerDiarization.from_pretrained(\"pyannote/speaker-diarization@2.1\", use_auth_token=HUGGINGFACE_API_KEY)\n",
        "\n",
        "# Move pipeline to GPU if available\n",
        "if torch.cuda.is_available():\n",
        "    pipeline.to(torch.device(\"cuda\"))\n",
        "    print(\"Pipeline is moved to GPU.\")\n",
        "else:\n",
        "    print(\"GPU not available. Using CPU.\")\n",
        "\n",
        "# Perform speaker diarization on the audio\n",
        "diarization = pipeline(audio_file_path)\n",
        "\n",
        "# Create a list to store the formatted conversation\n",
        "conversation = []\n",
        "\n",
        "# Iterate through diarization and match with transcription segments\n",
        "for turn, _, speaker in diarization.itertracks(yield_label=True):\n",
        "    # Convert the start and end times to integer indices\n",
        "    start_time = int(turn.start)\n",
        "    end_time = int(turn.end)\n",
        "\n",
        "    # Match the transcription segments by checking if the timestamps overlap with diarization times\n",
        "    segment_text = \"\"\n",
        "    for segment in segments:\n",
        "        if segment['start'] >= start_time and segment['end'] <= end_time:\n",
        "            segment_text += segment['text'] + \" \"\n",
        "\n",
        "    # Append the formatted conversation\n",
        "    conversation.append(f\"Speaker {speaker}: {segment_text.strip()}\")\n",
        "\n",
        "# Print the formatted conversation\n",
        "for entry in conversation:\n",
        "    print(entry)\n",
        "\n",
        "\n",
        "# After processing, move the pipeline back to CPU to free up GPU resources\n",
        "pipeline.to(torch.device(\"cpu\"))\n",
        "print(\"Pipeline is moved back to CPU.\")\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "5e02504f8b814be1b002523e2e1cd9a0",
            "b2ab43f91b954cb0a51781f4b36bc108",
            "14dcce591caa42cebff3558d457958a8",
            "94b87c9f16404e1082c89353bea816fd",
            "10c0dfb72a9246a49c4780e645512dde",
            "f95f904983774646863fea71c1c6c76c",
            "a13b73540905490e9035981af329312f",
            "4c6da3e5c35d4177b302dc5408cca0cf",
            "420909b64ba7413685a189548f8a36ba",
            "4eb43d0663084bc3bc66a640d624a47a",
            "3f420f867d7e4b4495a9c80f1f11ca0c",
            "323808a8527f4079bee87f92c37ddeab",
            "03bb82f56371496cb5b68a0e4d3a3e28",
            "0c796922cc9a478c94fd4cda875e5593",
            "e204339cf24f4b93a9edd6d8f47d1c16",
            "5e8258aa57224819826fce16626296e8",
            "2ffb5e7e443c49a0b589bcb75ba1e71c",
            "f31f74e86e28430cab97303986fd118f",
            "0369fdba01f34486a19f0e2255fed488",
            "32ec2ef44e954b07b60c6ca6400de6db",
            "f347b7a06310454095d6a248477b2eed",
            "672a0e95b56f438080055350d843f977",
            "6bbc64d26668480d9bc5a421cd9c30e5",
            "4fac67d5d5be4f7f949d819514e2ab80",
            "7cb03a455bcc4d3990507a44f61600f9",
            "63dc980659ed49388c74c7e499de2a22",
            "a73bdb4bee1f4a848759581138bb4168",
            "45abaa736d364253be4226ff64128d48",
            "b006a7c40f074d4fb66ee8ac52d1d631",
            "6252117fd4cd4bbd8d4b4baf2126cdd5",
            "2e03229792e34f6a98cfd8ed8fea19d5",
            "9067bc251c6f437c979bb37b868210f6",
            "2789f9c23dfb4a1580025f86b24cc2a1",
            "4f5e0c2c0dee44bea3bdef1c1fb46981",
            "30a62005b57f46f0aaac72babd4d697a",
            "78da01860b9b424c843a02eb10ae52d1",
            "2e58b4e02b5442068d70be85da196ac3",
            "420ad2cd1f8549449f2d8dfe716026cd",
            "4e8ffb5fa219434f944bfc43f8dbfc50",
            "f148ab88cacd450aa36ead6b58a863e0",
            "257cd76f15b14e1facc690e589cb5453",
            "67c6784125cf4af2a844d2690899a311",
            "b00981b8de2b47ef97f5dff6fa0fdca7",
            "01207ab6c78944c89c1fc297ed483ac6",
            "ae28d6cb80a14b1fa90118d48baa20bc",
            "4a9add1ee7c14d58803687b5a95c206d",
            "38dc4ee1870a4e5dafa64e1046cc8bc1",
            "6d1cb4fa215d41cdb22e787afcfd75b9",
            "f6fa4c5e05e14c5abce9146cb0c24864",
            "030a4fdbe64742a39ec70ce1751e77fd",
            "801ed635cf9242db98f1b607bad8bb35",
            "f95765dffcfe4cdd9bf9d44efb0c2d09",
            "4dd99cac0b10426d81a48b43c83d28d6",
            "2bb7ff6fea6e425bb28ba0e5d192df6b",
            "16096622652844708bf93a124836b69f",
            "eb78abe015f94c2fa2aecced8078c17c",
            "da2b3bdc5c1d4279b050c11f0b5dc48c",
            "0d82c490c3b8409faf0c5bd2ed858072",
            "fb8148cd2b984b23b04f54cfdb7b4ac8",
            "07535354d2ff4abc8d41dfa0b4110cb4",
            "148d66152bb94c319ee97a978efa2615",
            "573ea25d972a452c81f9638b966b94f7",
            "57300fd0c44e4decb994be2c5ca18318",
            "191050ed2c974d7fa403c02e780c4411",
            "32600471ad7d4c21a60a7b88115a662e",
            "e2c26531334a4a66872e1d4eb7f4d39f",
            "6a448d22a99a4cf182eed44a0be79cb5",
            "151f8f12fbc748d78812e0d54f598050",
            "6039415ea2a84ebb8e4c81d82dcc07c2",
            "fad1da77ac59480bb4759c7b5e6acdba",
            "8bb253bbc68d49b4aedf0cf790bae91c",
            "4b20871e06ce4a7797c6b4b6de2c61dc",
            "660f5e4bf16f4192a9cba178cbf4888d",
            "491144e55a1046f3811ecc6f0f58795e",
            "622480fb2b51466896d4005b5152b7c8",
            "031569b83055412c82b3bf41a66c2053",
            "224f2a050c694ba4827d8891305cbc0c",
            "2c7807a263d04e429fd7ad588ba3ce30",
            "71c66c93124f4343b9c66560cd8925e5",
            "90368cc547eb47f9af524b78f5a51191",
            "c4743eb012fa4481a5555cb75083ce50",
            "8dbd8bef28764adfa6d718288f05aab4",
            "cb35b3ceb6f7444d8ee77b8b8be99abc",
            "c02eb1b21f4149f29bc8bdbdaebf4b5a",
            "239273dfab89424596cc5d428b197c22",
            "d27651b1b44543d39194c6d6b8272456",
            "c58fb7ca4bff40f39ee9f465bc6f29a3",
            "6898ebb75553452d9b77b732f0c36e35"
          ]
        },
        "id": "GuEbMpJINLrB",
        "outputId": "d67d2e11-7d6b-42e1-e829-e48a9d2063cc",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:speechbrain.utils.checkpoints:Registered checkpoint save hook for _speechbrain_save\n",
            "DEBUG:speechbrain.utils.checkpoints:Registered checkpoint load hook for _speechbrain_load\n",
            "DEBUG:speechbrain.utils.checkpoints:Registered checkpoint save hook for save\n",
            "DEBUG:speechbrain.utils.checkpoints:Registered checkpoint load hook for load\n",
            "DEBUG:speechbrain.utils.checkpoints:Registered checkpoint save hook for _save\n",
            "DEBUG:speechbrain.utils.checkpoints:Registered checkpoint load hook for _recover\n",
            "100%|████████████████████████████████████████| 139M/139M [00:00<00:00, 369MiB/s]\n",
            "/usr/local/lib/python3.11/dist-packages/whisper/transcribe.py:176: UserWarning: Word-level timestamps on translations may not be reliable.\n",
            "  warnings.warn(\"Word-level timestamps on translations may not be reliable.\")\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.yaml:   0%|          | 0.00/500 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5e02504f8b814be1b002523e2e1cd9a0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/17.7M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "323808a8527f4079bee87f92c37ddeab"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.yaml:   0%|          | 0.00/318 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6bbc64d26668480d9bc5a421cd9c30e5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.migration.utils:Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.5.1.post0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../root/.cache/torch/pyannote/models--pyannote--segmentation/snapshots/c4c8ceafcbb3a7a280c2d357aee9fbc9b0be7f9b/pytorch_model.bin`\n",
            "INFO:speechbrain.utils.fetching:Fetch hyperparams.yaml: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model was trained with pyannote.audio 0.0.1, yours is 3.3.2. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
            "Model was trained with torch 1.10.0+cu102, yours is 2.6.0+cu124. Bad things might happen unless you revert torch to 1.x.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "hyperparams.yaml:   0%|          | 0.00/1.92k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4f5e0c2c0dee44bea3bdef1c1fb46981"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:speechbrain.utils.fetching:Fetch: Local file found, creating symlink '/root/.cache/huggingface/hub/models--speechbrain--spkrec-ecapa-voxceleb/snapshots/0f99f2d0ebe89ac095bcc5903c4dd8f72b367286/hyperparams.yaml' -> '/root/.cache/torch/pyannote/speechbrain/hyperparams.yaml'\n",
            "INFO:speechbrain.utils.fetching:Fetch custom.py: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n",
            "DEBUG:speechbrain.utils.checkpoints:Registered checkpoint save hook for _save\n",
            "DEBUG:speechbrain.utils.checkpoints:Registered checkpoint load hook for _load\n",
            "DEBUG:speechbrain.utils.checkpoints:Registered parameter transfer hook for _load\n",
            "/usr/local/lib/python3.11/dist-packages/speechbrain/utils/autocast.py:188: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "  wrapped_fwd = torch.cuda.amp.custom_fwd(fwd, cast_inputs=cast_inputs)\n",
            "DEBUG:speechbrain.utils.checkpoints:Registered checkpoint save hook for save\n",
            "DEBUG:speechbrain.utils.checkpoints:Registered checkpoint load hook for load_if_possible\n",
            "DEBUG:speechbrain.utils.parameter_transfer:Collecting files (or symlinks) for pretraining in /root/.cache/torch/pyannote/speechbrain.\n",
            "INFO:speechbrain.utils.fetching:Fetch embedding_model.ckpt: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "embedding_model.ckpt:   0%|          | 0.00/83.3M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ae28d6cb80a14b1fa90118d48baa20bc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:speechbrain.utils.fetching:Fetch: Local file found, creating symlink '/root/.cache/huggingface/hub/models--speechbrain--spkrec-ecapa-voxceleb/snapshots/0f99f2d0ebe89ac095bcc5903c4dd8f72b367286/embedding_model.ckpt' -> '/root/.cache/torch/pyannote/speechbrain/embedding_model.ckpt'\n",
            "DEBUG:speechbrain.utils.parameter_transfer:Set local path in self.paths[\"embedding_model\"] = /root/.cache/torch/pyannote/speechbrain/embedding_model.ckpt\n",
            "INFO:speechbrain.utils.fetching:Fetch mean_var_norm_emb.ckpt: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "mean_var_norm_emb.ckpt:   0%|          | 0.00/1.92k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "eb78abe015f94c2fa2aecced8078c17c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:speechbrain.utils.fetching:Fetch: Local file found, creating symlink '/root/.cache/huggingface/hub/models--speechbrain--spkrec-ecapa-voxceleb/snapshots/0f99f2d0ebe89ac095bcc5903c4dd8f72b367286/mean_var_norm_emb.ckpt' -> '/root/.cache/torch/pyannote/speechbrain/mean_var_norm_emb.ckpt'\n",
            "DEBUG:speechbrain.utils.parameter_transfer:Set local path in self.paths[\"mean_var_norm_emb\"] = /root/.cache/torch/pyannote/speechbrain/mean_var_norm_emb.ckpt\n",
            "INFO:speechbrain.utils.fetching:Fetch classifier.ckpt: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "classifier.ckpt:   0%|          | 0.00/5.53M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6a448d22a99a4cf182eed44a0be79cb5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:speechbrain.utils.fetching:Fetch: Local file found, creating symlink '/root/.cache/huggingface/hub/models--speechbrain--spkrec-ecapa-voxceleb/snapshots/0f99f2d0ebe89ac095bcc5903c4dd8f72b367286/classifier.ckpt' -> '/root/.cache/torch/pyannote/speechbrain/classifier.ckpt'\n",
            "DEBUG:speechbrain.utils.parameter_transfer:Set local path in self.paths[\"classifier\"] = /root/.cache/torch/pyannote/speechbrain/classifier.ckpt\n",
            "INFO:speechbrain.utils.fetching:Fetch label_encoder.txt: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "label_encoder.txt:   0%|          | 0.00/129k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2c7807a263d04e429fd7ad588ba3ce30"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:speechbrain.utils.fetching:Fetch: Local file found, creating symlink '/root/.cache/huggingface/hub/models--speechbrain--spkrec-ecapa-voxceleb/snapshots/0f99f2d0ebe89ac095bcc5903c4dd8f72b367286/label_encoder.txt' -> '/root/.cache/torch/pyannote/speechbrain/label_encoder.ckpt'\n",
            "DEBUG:speechbrain.utils.parameter_transfer:Set local path in self.paths[\"label_encoder\"] = /root/.cache/torch/pyannote/speechbrain/label_encoder.ckpt\n",
            "INFO:speechbrain.utils.parameter_transfer:Loading pretrained files for: embedding_model, mean_var_norm_emb, classifier, label_encoder\n",
            "DEBUG:speechbrain.utils.parameter_transfer:Redirecting (loading from local path): embedding_model -> /root/.cache/torch/pyannote/speechbrain/embedding_model.ckpt\n",
            "DEBUG:speechbrain.utils.parameter_transfer:Redirecting (loading from local path): mean_var_norm_emb -> /root/.cache/torch/pyannote/speechbrain/mean_var_norm_emb.ckpt\n",
            "DEBUG:speechbrain.utils.parameter_transfer:Redirecting (loading from local path): classifier -> /root/.cache/torch/pyannote/speechbrain/classifier.ckpt\n",
            "DEBUG:speechbrain.utils.parameter_transfer:Redirecting (loading from local path): label_encoder -> /root/.cache/torch/pyannote/speechbrain/label_encoder.ckpt\n",
            "DEBUG:speechbrain.dataio.encoder:Loaded categorical encoding from /root/.cache/torch/pyannote/speechbrain/label_encoder.ckpt\n",
            "INFO:speechbrain.utils.fetching:Fetch hyperparams.yaml: Using symlink found at '/root/.cache/torch/pyannote/speechbrain/hyperparams.yaml'\n",
            "INFO:speechbrain.utils.fetching:Fetch custom.py: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n",
            "DEBUG:speechbrain.utils.parameter_transfer:Collecting files (or symlinks) for pretraining in /root/.cache/torch/pyannote/speechbrain.\n",
            "INFO:speechbrain.utils.fetching:Fetch embedding_model.ckpt: Using symlink found at '/root/.cache/torch/pyannote/speechbrain/embedding_model.ckpt'\n",
            "DEBUG:speechbrain.utils.parameter_transfer:Set local path in self.paths[\"embedding_model\"] = /root/.cache/torch/pyannote/speechbrain/embedding_model.ckpt\n",
            "INFO:speechbrain.utils.fetching:Fetch mean_var_norm_emb.ckpt: Using symlink found at '/root/.cache/torch/pyannote/speechbrain/mean_var_norm_emb.ckpt'\n",
            "DEBUG:speechbrain.utils.parameter_transfer:Set local path in self.paths[\"mean_var_norm_emb\"] = /root/.cache/torch/pyannote/speechbrain/mean_var_norm_emb.ckpt\n",
            "INFO:speechbrain.utils.fetching:Fetch classifier.ckpt: Using symlink found at '/root/.cache/torch/pyannote/speechbrain/classifier.ckpt'\n",
            "DEBUG:speechbrain.utils.parameter_transfer:Set local path in self.paths[\"classifier\"] = /root/.cache/torch/pyannote/speechbrain/classifier.ckpt\n",
            "INFO:speechbrain.utils.fetching:Fetch label_encoder.txt: Using symlink found at '/root/.cache/torch/pyannote/speechbrain/label_encoder.ckpt'\n",
            "DEBUG:speechbrain.utils.parameter_transfer:Set local path in self.paths[\"label_encoder\"] = /root/.cache/torch/pyannote/speechbrain/label_encoder.ckpt\n",
            "INFO:speechbrain.utils.parameter_transfer:Loading pretrained files for: embedding_model, mean_var_norm_emb, classifier, label_encoder\n",
            "DEBUG:speechbrain.utils.parameter_transfer:Redirecting (loading from local path): embedding_model -> /root/.cache/torch/pyannote/speechbrain/embedding_model.ckpt\n",
            "DEBUG:speechbrain.utils.parameter_transfer:Redirecting (loading from local path): mean_var_norm_emb -> /root/.cache/torch/pyannote/speechbrain/mean_var_norm_emb.ckpt\n",
            "DEBUG:speechbrain.utils.parameter_transfer:Redirecting (loading from local path): classifier -> /root/.cache/torch/pyannote/speechbrain/classifier.ckpt\n",
            "DEBUG:speechbrain.utils.parameter_transfer:Redirecting (loading from local path): label_encoder -> /root/.cache/torch/pyannote/speechbrain/label_encoder.ckpt\n",
            "DEBUG:speechbrain.dataio.encoder:Loaded categorical encoding from /root/.cache/torch/pyannote/speechbrain/label_encoder.ckpt\n",
            "/usr/local/lib/python3.11/dist-packages/pyannote/audio/utils/reproducibility.py:74: ReproducibilityWarning: TensorFloat-32 (TF32) has been disabled as it might lead to reproducibility issues and lower accuracy.\n",
            "It can be re-enabled by calling\n",
            "   >>> import torch\n",
            "   >>> torch.backends.cuda.matmul.allow_tf32 = True\n",
            "   >>> torch.backends.cudnn.allow_tf32 = True\n",
            "See https://github.com/pyannote/pyannote-audio/issues/1370 for more details.\n",
            "\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pipeline is moved to GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchaudio/_backend/soundfile_backend.py:71: UserWarning: The MPEG_LAYER_III subtype is unknown to TorchAudio. As a result, the bits_per_sample attribute will be set to 0. If you are seeing this warning, please report by opening an issue on github (after checking for existing/closed ones). You may otherwise ignore this warning.\n",
            "  warnings.warn(\n",
            "INFO:speechbrain.utils.fetching:Fetch hyperparams.yaml: Using symlink found at '/root/.cache/torch/pyannote/speechbrain/hyperparams.yaml'\n",
            "INFO:speechbrain.utils.fetching:Fetch custom.py: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Speaker SPEAKER_00: \n",
            "Speaker SPEAKER_00: And if like the user is not having a existing account on DG locker,  so like here you go for the sign up page and there are long process of like  loading the docs on the like on the docs on the DG locker and then again you will give the  consent of like whatever documents he is like likely to provide those room cards further we can\n",
            "Speaker SPEAKER_01: \n",
            "Speaker SPEAKER_01: \n",
            "Speaker SPEAKER_01: Exactly.\n",
            "Speaker SPEAKER_00: So I just wanted to one thing, there's one thing, like here if like in the DG locker you have an option to  open multiple documents like a driving and some app and card and what I was saying,  Adhalka, 10th mark, 10th mark, 10th mark, okay and many more documents you can upload on the  app itself. So like ARC use case we only have two to three that document they put in for us like  Adhalka, Devin license or in any case fan card okay. So like can we have do we have an option that  while he is signing up so only those documents are able to read the width of the upload there and  that stuff like he can ignore and like how we will be like able to like are you going to  provide that information to the user itself before he is like you actually do the review or something.\n",
            "Speaker SPEAKER_01: \n",
            "Speaker SPEAKER_01: So we have we have multiple closer depending upon what not log you know what to verify.  You can so I'll actually share this document with you. I am sharing my thing. So we are\n",
            "Speaker SPEAKER_00: \n",
            "Speaker SPEAKER_00: \n",
            "Speaker SPEAKER_01: So it has everything all the collaterals that we have. Solution number one is whether the  customer does not agree Adhalka is a DG locker or not. I've written the solution. If this  problem is getting a DG locker password we have that. Now the question that you come into  DG locker if we if you don't need all the documents we have multiple closer.  The portal DG locker prove the gay we have multiple if we want just on the Adha we have this we  have you want Adha and Pan we have this and we have what right driving license.\n",
            "Speaker SPEAKER_00: \n",
            "Speaker SPEAKER_00: \n",
            "Speaker SPEAKER_01: \n",
            "Speaker SPEAKER_00: \n",
            "Speaker SPEAKER_01: \n",
            "Speaker SPEAKER_01: \n",
            "Speaker SPEAKER_01: \n",
            "Speaker SPEAKER_02: the curve would only be like. Nice. I think care of this testing plus integration.  So we would want to that would be great actually we would want to test the APIs first and see how it  will set up or fit into our flow and based on that then we can take it forward from there.\n",
            "Speaker SPEAKER_00: \n",
            "Speaker SPEAKER_01: \n",
            "Speaker SPEAKER_02: \n",
            "Speaker SPEAKER_01: \n",
            "Speaker SPEAKER_01: Success rates result in different definitions to define success rates. So I am a gay company.  Your success rates and I define for my success whether I'm giving you a 200 response or not.\n",
            "Speaker SPEAKER_02: \n",
            "Speaker SPEAKER_00: \n",
            "Speaker SPEAKER_01: The first one. I will give you some basic example here. For example you're giving me a hard\n",
            "Speaker SPEAKER_00: \n",
            "Speaker SPEAKER_01: So the whole job of the API is whether to verify the given Alharkan. If you give me a correct  Alharkan the API will obviously give you a 200 response and tell you that this is a valid Alharkan.\n",
            "Speaker SPEAKER_02: \n",
            "Speaker SPEAKER_01: \n",
            "Speaker SPEAKER_02: \n",
            "Speaker SPEAKER_00: \n",
            "Speaker SPEAKER_01: This is not a valid Alharkan.  I'll go through it but I might experience actually 97 more than 97% of the roads are there.\n",
            "Speaker SPEAKER_02: But that's on the response. That's on the response receiving a valid response.\n",
            "Speaker SPEAKER_01: \n",
            "Speaker SPEAKER_02: \n",
            "Speaker SPEAKER_01: The success rate. No, no, no, giving you hitting the API successfully.  Yes. But hitting the API success rate.\n",
            "Speaker SPEAKER_00: Yes. But hitting the API success rate.  Basically there are two cases. One is the case. If the government upside is down.  Like the source where you are catching the documents. Second, it's like if descent to  go down. So if government server shows it down then definitely we are ignoring that.  Other than that, how much success rate of descent to not getting failed?  After hitting the government.\n",
            "Speaker SPEAKER_01: \n",
            "Speaker SPEAKER_01: That's more than 99% sir. All the downsides that we have, we have scheduled downsides.  It will be part of the agreement also. We should know before you know seven days.\n",
            "Speaker SPEAKER_00: \n",
            "Speaker SPEAKER_00: \n",
            "Speaker SPEAKER_02: \n",
            "Speaker SPEAKER_02: Yes.\n",
            "Speaker SPEAKER_01: \n",
            "Speaker SPEAKER_01: \n",
            "Speaker SPEAKER_01: \n",
            "Speaker SPEAKER_00: \n",
            "Speaker SPEAKER_01: \n",
            "Speaker SPEAKER_01: \n",
            "Speaker SPEAKER_01: \n",
            "Speaker SPEAKER_00: \n",
            "Speaker SPEAKER_01: \n",
            "Speaker SPEAKER_01: \n",
            "Speaker SPEAKER_02: \n",
            "Speaker SPEAKER_02: \n",
            "Speaker SPEAKER_01: \n",
            "Speaker SPEAKER_00: \n",
            "Speaker SPEAKER_00: individual of the API.  We can type those APIs.\n",
            "Speaker SPEAKER_02: local from your site, how many are able to successfully get the documents? Any idea on that?\n",
            "Speaker SPEAKER_00: \n",
            "Speaker SPEAKER_02: Successfully, the document successfully come up.\n",
            "Speaker SPEAKER_00: \n",
            "Speaker SPEAKER_01: I think I'll have an idea to the dashboard. I'll check it out for the team. They have access to this.\n",
            "Speaker SPEAKER_02: \n",
            "Speaker SPEAKER_02: Because we already have like the Adhar API, we already have that.  I told you also we have integrated ID central right now.  So we have those Adhar APIs to verify and we also have direct license API based on like getting the  user like the user would input the driving license number and we fetch the details of the user.  So we those APIs, we have some success rate around it.  I want to compare that success rate with digital occur success rate also.  Maybe it would be less, but still I would want to know how much would we be able to benefit?  Because one why we want to integrate digital occur is also you would definitely be knowing that  the same device ID will have digital occur installed.  So if we so right now there are still like challenges when a person who is using a different account is able to verify Adhar of somebody else.  But that situation would not come in case of digital occur.\n",
            "Speaker SPEAKER_01: \n",
            "Speaker SPEAKER_02: If how many users come for the sign-in sign-up process and how many are actually able to validate digital occur?\n",
            "Speaker SPEAKER_01: No, definitely that makes sense.  Other scientists, the standalone API, like you mentioned for Adhar and I like that you also have that.\n",
            "Speaker SPEAKER_02: Sure, sure. Yeah, probably right now we are looking at digital occur.  So I would just want to see how much.\n",
            "Speaker SPEAKER_01: \n",
            "Speaker SPEAKER_01: If you can just mark me an email with these monitors.  Okay.\n",
            "Speaker SPEAKER_02: \n",
            "Speaker SPEAKER_00: \n",
            "Speaker SPEAKER_01: Yeah, I want you to volunteer.  If you can just help, I will get a full name from here.\n",
            "Speaker SPEAKER_02: \n",
            "Speaker SPEAKER_01: \n",
            "Speaker SPEAKER_02: Mobile number, yeah, sure.\n",
            "Speaker SPEAKER_02: \n",
            "Speaker SPEAKER_02: \n",
            "Speaker SPEAKER_02: \n",
            "Speaker SPEAKER_01: Okay.\n",
            "Speaker SPEAKER_01: \n",
            "Speaker SPEAKER_01: \n",
            "Speaker SPEAKER_00: \n",
            "Speaker SPEAKER_01: It's me or he says me or did the word gives me your and also gives me a passport number.  I don't know which idea is this, but this is some writing to pass.  So this is your addresses, your two pass one.\n",
            "Speaker SPEAKER_02: \n",
            "Speaker SPEAKER_02: Yeah, one one with cancel, right?  So both the pass.\n",
            "Speaker SPEAKER_01: \n",
            "Speaker SPEAKER_02: Wow.\n",
            "Speaker SPEAKER_01: Then your phone number is the email ID there and this is your summary.  It's summary miss, right?  You'll get your score.  This is your score.  Okay.\n",
            "Speaker SPEAKER_01: Okay.  I had to show you this because I don't have the address so the just the score.  But you would want just the score.  Yeah.\n",
            "Speaker SPEAKER_02: \n",
            "Speaker SPEAKER_02: \n",
            "Speaker SPEAKER_01: \n",
            "Speaker SPEAKER_02: Name is also.\n",
            "Speaker SPEAKER_01: Name is also.\n",
            "Speaker SPEAKER_01: \n",
            "Speaker SPEAKER_00: Okay.  All the name are that these both are the mandatory cases.\n",
            "Speaker SPEAKER_01: These are both.  Yeah, these are my.\n",
            "Speaker SPEAKER_02: Okay.  The name and mobile number.  Phone number we have from the guest initially because he logs in to zoom car and then we can go forward.  But my name.\n",
            "Speaker SPEAKER_01: \n",
            "Speaker SPEAKER_01: I can go on both.\n",
            "Speaker SPEAKER_01: \n",
            "Speaker SPEAKER_00: \n",
            "Speaker SPEAKER_01: There's another API.\n",
            "Speaker SPEAKER_01: Yeah, just type your mobile on.\n",
            "Speaker SPEAKER_01: \n",
            "Speaker SPEAKER_01: \n",
            "Speaker SPEAKER_00: \n",
            "Speaker SPEAKER_01: \n",
            "Speaker SPEAKER_01: \n",
            "Speaker SPEAKER_00: \n",
            "Speaker SPEAKER_01: I get your bank register name.\n",
            "Speaker SPEAKER_00: \n",
            "Speaker SPEAKER_01: You would you could be doing a penny drop in the account.\n",
            "Speaker SPEAKER_02: \n",
            "Speaker SPEAKER_01: Also, but this is like another level of security that can be done.  So your step one can be an input as little as mobile number.  Because someone like the grow is doing this with me.  Many big trading companies grow up and start trading there and move by.  Is it all that I have to know now?  I was trying to pitch it but they were on an end house.  So what these guys do is they just get your phone number.  Phone number is using the save here.  They'll get your name back account details.\n",
            "Speaker SPEAKER_00: \n",
            "Speaker SPEAKER_02: Okay.\n",
            "Speaker SPEAKER_01: And again, the last flow would be where the customer has to complete his digital offer.  Digital offer is only input was just a mobile number.\n",
            "Speaker SPEAKER_00: \n",
            "Speaker SPEAKER_01: The manual intervention is giving you a TV.\n",
            "Speaker SPEAKER_02: Okay.\n",
            "Speaker SPEAKER_01: \n",
            "Speaker SPEAKER_01: What's the mobile number and you are validating so many other documents.\n",
            "Speaker SPEAKER_02: \n",
            "Speaker SPEAKER_01: \n",
            "Speaker SPEAKER_02: The postman, the one that you showed earlier.\n",
            "Speaker SPEAKER_01: \n",
            "Speaker SPEAKER_01: That is like a mandatory benefit.  I cannot be.\n",
            "Speaker SPEAKER_02: \n",
            "Speaker SPEAKER_01: \n",
            "Speaker SPEAKER_01: \n",
            "Speaker SPEAKER_01: So that's yeah.\n",
            "Speaker SPEAKER_02: Name name is not.  Okay.\n",
            "Speaker SPEAKER_01: \n",
            "Speaker SPEAKER_00: One character was summary.  One more.  There was no commission.\n",
            "Speaker SPEAKER_01: \n",
            "Speaker SPEAKER_00: So many like two APS.  One was the credit score.  And the other was the full information.  Like the score pen card.  Yes.  Yes.\n",
            "Speaker SPEAKER_01: Yes.  Yes.\n",
            "Speaker SPEAKER_00: \n",
            "Speaker SPEAKER_01: First one was this.\n",
            "Speaker SPEAKER_00: \n",
            "Speaker SPEAKER_00: Okay.\n",
            "Speaker SPEAKER_01: Here the mandatory inputs are named.  What I will buy.  This once would look something like this.  I don't know Vijay Malaya.  So here looks like this.  So your name ystems and you.  我的同 밑 ABAP点被 那 He's  30icated items  oudies マナダル  In order to make everyone.\n",
            "Speaker SPEAKER_00: \n",
            "Speaker SPEAKER_02: \n",
            "Speaker SPEAKER_02: \n",
            "Speaker SPEAKER_01: aucune.\n",
            "Speaker SPEAKER_02: \n",
            "Speaker SPEAKER_02: \n",
            "Speaker SPEAKER_01: \n",
            "Speaker SPEAKER_02: Sure, it should be a soft tool.\n",
            "Speaker SPEAKER_01: This is the all data soft tool.  We have three.  Just this or summary and detailed.  All three are soft tools.\n",
            "Speaker SPEAKER_02: \n",
            "Speaker SPEAKER_01: And this is the actual let's do one thing.  Let me give you pre-prod to the extra.  Otherwise, this is the same election end time.\n",
            "Speaker SPEAKER_00: \n",
            "Speaker SPEAKER_01: Let's, I want you to test it on live data.  So, I will try to get pre-prod access.  This may not you will have to give me or even setting IP address.  Be right listed.\n",
            "Speaker SPEAKER_01: Staging my IP by listing is not required, but we will again go.  Simulation end time.  It's like a lot of things.\n",
            "Speaker SPEAKER_01: I want you to try it on board.  You have to give me an internet static.  I will list and then you hit from the same IP address.\n",
            "Speaker SPEAKER_02: This will have to check if you can do that.  No, it's not like that.\n",
            "Speaker SPEAKER_00: No, it's not like that.  It's like a game.\n",
            "Speaker SPEAKER_01: But if I give you a staging, then staging is a data much sensitive.  So, the data that you can do in a digital or live data state is not a problem.  But the data that you can do in a digital sense is not a problem.  So, we usually test it on a single level.  You can make a simulation data.  You can make a negative or positive basis.\n",
            "Speaker SPEAKER_00: \n",
            "Speaker SPEAKER_01: Just to see that IP address is required.  But I want, but this is my last month's care.  You can do a live data.  I don't know.  I'm not going to do anything.\n",
            "Speaker SPEAKER_02: \n",
            "Speaker SPEAKER_00: \n",
            "Speaker SPEAKER_00: You can ask for a case.  You can find an endpoint.  We need to find an endpoint.\n",
            "Speaker SPEAKER_01: You can find an endpoint.  You can use a Windows static IP address.\n",
            "Speaker SPEAKER_00: Yes, I can request that.\n",
            "Speaker SPEAKER_01: \n",
            "Speaker SPEAKER_00: You can request that.\n",
            "Speaker SPEAKER_01: You can request that.\n",
            "Speaker SPEAKER_00: And that case, I will provide that.\n",
            "Speaker SPEAKER_01: \n",
            "Speaker SPEAKER_00: I can request that.   Okay, okay.  Okay, fine.  Okay, okay.\n",
            "Speaker SPEAKER_02: \n",
            "Speaker SPEAKER_02: Credit school.  Yes, yes.  I can request that.\n",
            "Speaker SPEAKER_00: \n",
            "Speaker SPEAKER_01: An art.  An art documentation.  You can all explain the whole API collection.\n",
            "Speaker SPEAKER_00: Okay.  After this.\n",
            "Speaker SPEAKER_01: And send it.\n",
            "Speaker SPEAKER_02: \n",
            "Speaker SPEAKER_01: \n",
            "Speaker SPEAKER_02: Just let us know the credit.\n",
            "Speaker SPEAKER_01: The credits are 10, I guess.  I will see if you exhaust the credits.  You just ping me on what's up.\n",
            "Speaker SPEAKER_00: \n",
            "Speaker SPEAKER_02: Okay.\n",
            "Speaker SPEAKER_01: Sure.  Okay.  I want to understand the volume.\n",
            "Speaker SPEAKER_02: Yeah.  Volume.  I'll give you.  I'm also checking.  I think my dashboard is down right now.  I was thinking of getting those documents before the meeting.  But I was not able to get the volumes.  I will give you in a probably another one hour.  I'll message you on what's up.\n",
            "Speaker SPEAKER_01: Good.  Yeah.  I want to give you a proposal also something.\n",
            "Speaker SPEAKER_02: \n",
            "Speaker SPEAKER_02: Sure.  I'll give you for a profile verification daily volume kind of.  I'll give you.  And for profile verification in the sense for digital occur.  And for credit score check also.\n",
            "Speaker SPEAKER_01: \n",
            "Speaker SPEAKER_01: So if you please.  And documentation.  So completely.  And you will be sending with that.  What does.  And I have to.  Open up a product.  I want to.\n",
            "Speaker SPEAKER_00: \n",
            "Speaker SPEAKER_00: \n",
            "Speaker SPEAKER_02: \n",
            "Speaker SPEAKER_01: \n",
            "Speaker SPEAKER_02: Okay.  Yeah.  Got it.  Any estimate.  You.  You will not be able to.  Okay.  Let me give you the volume.  I think we'll talk about.\n",
            "Speaker SPEAKER_01: I give you the volume.  First.  We have.  And.  Okay.\n",
            "Speaker SPEAKER_02: \n",
            "Speaker SPEAKER_01: \n",
            "Speaker SPEAKER_02: Yeah.\n",
            "Speaker SPEAKER_02: \n",
            "Speaker SPEAKER_00: \n",
            "Speaker SPEAKER_00: So.  Right.  Like.\n",
            "Speaker SPEAKER_00: I because.  Other.  If I like first day.  I don't know.  It's.  I will.  Data.  Like.  I like.  We can.  I slow that one.  So that.  We are using.  Other multiple.  Yeah.  So get the details.    Right.\n",
            "Speaker SPEAKER_01: Right.  We have another.  You.   Now.   Look.  I.\n",
            "Speaker SPEAKER_02: \n",
            "Speaker SPEAKER_00: I.\n",
            "Speaker SPEAKER_00: The.  I.\n",
            "Speaker SPEAKER_00: \n",
            "Speaker SPEAKER_01: Just.  Okay.  Instead of generate.  Report.  Yeah.  Yeah.  You.  Sure.  You.\n",
            "Speaker SPEAKER_02: \n",
            "Speaker SPEAKER_01: It's there.  We don't involve financials, but you'll get all the K-Pi  data starting from data for gender,  for numbers, email address, fan number,  passport number, sharing like so on.\n",
            "Speaker SPEAKER_02: \n",
            "Speaker SPEAKER_02: Okay.  Sounds good.\n",
            "Speaker SPEAKER_01: And then, I'll add a little address.  Yeah.  We don't get financials.  So, combination of these two APS would be better  than the word rate about APS.  Makes sense.  I'll do it.  Which is good.\n",
            "Speaker SPEAKER_00: Makes sense.  I'll do it.  Which is good.  So, my summary.  This last thing I got, I have a customer data pool.  I know.  I want to get this kind of dog.  I mean, that.\n",
            "Speaker SPEAKER_00: We can just slow it.  I'll let you know that we have to test it.\n",
            "Speaker SPEAKER_00: Or, we'll just...\n",
            "Speaker SPEAKER_01: \n",
            "Speaker SPEAKER_01: \n",
            "Speaker SPEAKER_00: Okay.\n",
            "Speaker SPEAKER_00: Okay.   Well.\n",
            "Speaker SPEAKER_01: I think the action is aligned.\n",
            "Speaker SPEAKER_02: Yes.  Yes, Sumrath.  Action equals are aligned.\n",
            "Speaker SPEAKER_02: We'll do testing.  You provide us access.  You provide us access.  We'll do testing.  I will share with you the volume so that you can share the commercials.\n",
            "Speaker SPEAKER_01: Sure.  It's out.\n",
            "Speaker SPEAKER_01: And you will visit to office.  And another on six of my, my marketing team will be company.\n",
            "Speaker SPEAKER_02: Okay.  Let's see about that.  That's okay.  Thank you so much for the invitation.\n",
            "Speaker SPEAKER_01: \n",
            "Speaker SPEAKER_02: Yeah.\n",
            "Speaker SPEAKER_01: Okay.  Yeah.  That's good.\n",
            "Speaker SPEAKER_02: Yeah.\n",
            "Speaker SPEAKER_01: I think, yeah.  So, we're good to go.  Yeah.  Very nice.      Okay.  Look and forward.\n",
            "Speaker SPEAKER_02: Paul at all estamos.\n",
            "Speaker SPEAKER_01: Alright, thai.\n",
            "Speaker SPEAKER_02: \n",
            "Speaker SPEAKER_00: can we just준\n",
            "Speaker SPEAKER_01: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:speechbrain.utils.parameter_transfer:Collecting files (or symlinks) for pretraining in /root/.cache/torch/pyannote/speechbrain.\n",
            "INFO:speechbrain.utils.fetching:Fetch embedding_model.ckpt: Using symlink found at '/root/.cache/torch/pyannote/speechbrain/embedding_model.ckpt'\n",
            "DEBUG:speechbrain.utils.parameter_transfer:Set local path in self.paths[\"embedding_model\"] = /root/.cache/torch/pyannote/speechbrain/embedding_model.ckpt\n",
            "INFO:speechbrain.utils.fetching:Fetch mean_var_norm_emb.ckpt: Using symlink found at '/root/.cache/torch/pyannote/speechbrain/mean_var_norm_emb.ckpt'\n",
            "DEBUG:speechbrain.utils.parameter_transfer:Set local path in self.paths[\"mean_var_norm_emb\"] = /root/.cache/torch/pyannote/speechbrain/mean_var_norm_emb.ckpt\n",
            "INFO:speechbrain.utils.fetching:Fetch classifier.ckpt: Using symlink found at '/root/.cache/torch/pyannote/speechbrain/classifier.ckpt'\n",
            "DEBUG:speechbrain.utils.parameter_transfer:Set local path in self.paths[\"classifier\"] = /root/.cache/torch/pyannote/speechbrain/classifier.ckpt\n",
            "INFO:speechbrain.utils.fetching:Fetch label_encoder.txt: Using symlink found at '/root/.cache/torch/pyannote/speechbrain/label_encoder.ckpt'\n",
            "DEBUG:speechbrain.utils.parameter_transfer:Set local path in self.paths[\"label_encoder\"] = /root/.cache/torch/pyannote/speechbrain/label_encoder.ckpt\n",
            "INFO:speechbrain.utils.parameter_transfer:Loading pretrained files for: embedding_model, mean_var_norm_emb, classifier, label_encoder\n",
            "DEBUG:speechbrain.utils.parameter_transfer:Redirecting (loading from local path): embedding_model -> /root/.cache/torch/pyannote/speechbrain/embedding_model.ckpt\n",
            "DEBUG:speechbrain.utils.parameter_transfer:Redirecting (loading from local path): mean_var_norm_emb -> /root/.cache/torch/pyannote/speechbrain/mean_var_norm_emb.ckpt\n",
            "DEBUG:speechbrain.utils.parameter_transfer:Redirecting (loading from local path): classifier -> /root/.cache/torch/pyannote/speechbrain/classifier.ckpt\n",
            "DEBUG:speechbrain.utils.parameter_transfer:Redirecting (loading from local path): label_encoder -> /root/.cache/torch/pyannote/speechbrain/label_encoder.ckpt\n",
            "DEBUG:speechbrain.dataio.encoder:Loaded categorical encoding from /root/.cache/torch/pyannote/speechbrain/label_encoder.ckpt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pipeline is moved back to CPU.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "\n",
        "# Set your OpenAI API key\n",
        "openai.api_key = os.environ[\"OPENAI_API_KEY\"] # Replace 'your-openai-api-key' with your actual API key or better yet, store it as env variable\n",
        "\n",
        "# Function to generate the summary using GPT-3.5 or GPT-4\n",
        "def summarize_text(text):\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo\",  # Use gpt-3.5-turbo instead of the deprecated text-davinci-003\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a personal AI assistant who is hired to take notes and summarize meetings for future reference. Assume that the transcription provided is of a business meeting.\"},\n",
        "            {\"role\": \"user\", \"content\": f\"Summarize (within 500 tokens) the following conversation (from an audio recording) without missing any important details. Segregate if there are multiple topics discussed. Each topic with its heading and summary should start by mentioning who said it. If multiple speakers in one topic, then divide summary into paragraphs with each paragraph starting with which speaker mentioned about that summary para. In the suppary para, go one level deeper - what was the speaker actually trying to say. Do not include any follow up remarks/questions: {text}\"}\n",
        "        ],\n",
        "        max_tokens=500\n",
        "    )\n",
        "    return response.choices[0].message['content'].strip()\n",
        "\n",
        "# Assuming transcribed_text is the result from your transcription model (e.g., Whisper)\n",
        "# transcribed_text = \"Your transcribed text goes here\" #Use the output from Whisper instead of this hardcoded value\n",
        "# Assuming transcribed_text is already defined from your previous Whisper transcription step\n",
        "# transcribed_text = result['text']\n",
        "\n",
        "# Get the summary\n",
        "summary = summarize_text(conversation)\n",
        "print(\"Summary: \", summary)\n",
        "\n",
        "import torch\n",
        "torch.cuda.empty_cache()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y7lJaasg3KZO",
        "outputId": "a900712f-1893-4e84-9982-1dbbee6e22f7",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summary:  **Discussion on Document Upload Process in DG Locker:**\n",
            "- **Speaker SPEAKER_00** mentioned about the process of loading documents onto DG locker after signing up, including the consent required for document submission.\n",
            "- **Speaker SPEAKER_00** also inquired about customizing document upload options based on user needs during the sign-up process. Mentioned the need to inform users about document upload requirements beforehand.\n",
            "\n",
            "**Verification Process using APIs:**\n",
            "- **Speaker SPEAKER_02** discussed integration of Digital Locker APIs for identity verification, comparing success rates with existing Adhar and License APIs.\n",
            "- **Speaker SPEAKER_01** elaborated on defining and testing success rates concerning API responses like hitting the API successfully and receiving valid responses.\n",
            "\n",
            "**Security Measures and Data Verification:**\n",
            "- **Speaker SPEAKER_01** detailed security measures like using phone numbers for verification and addressing potential vulnerabilities like unauthorized access to user data.\n",
            "- **Speaker SPEAKER_00** emphasized the necessity of manual intervention and multi-level document verification for improved security checks.\n",
            "\n",
            "**Technical Integration and Testing:**\n",
            "- **Speaker SPEAKER_02** shared plans for API testing, need for endpoint access, and emphasizing pre-production testing prior to live data usage.\n",
            "- **Speaker SPEAKER_01** requested API documentation and testing on live data, highlighting the importance of security and simulation tests.\n",
            "\n",
            "**Data Collection and Reporting Features:**\n",
            "- **Speaker SPEAKER_01** discussed collecting user data for verification and generating reports including demographic details without financial data.\n",
            "- **Speaker SPEAKER_00** concurred on the value of document collection and emphasized the need for testing and communication on the next steps.\n",
            "\n",
            "**Agreement on Action Items:**\n",
            "- **Speaker SPEAKER_01** confirmed alignment on next steps for testing and data sharing.\n",
            "- **Speaker SPEAKER_02** acknowledged the testing plan and expressed thanks for the collaboration.\n",
            "\n",
            "**Closing Remarks and Future Plans:**\n",
            "- The speakers discussed visiting the office and future collaborations, ensuring alignment on upcoming actions and expressing positive sentiments towards future collaboration and development.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import smtplib\n",
        "from email.mime.multipart import MIMEMultipart\n",
        "from email.mime.text import MIMEText\n",
        "import os\n",
        "\n",
        "# Function to send email\n",
        "def send_email(conversation, summary):\n",
        "    # Set up the server\n",
        "    smtp_server = \"smtp.gmail.com\"  # Using Gmail's SMTP server\n",
        "    smtp_port = 587  # Standard SMTP port for TLS\n",
        "    sender_email = \"vashishta.pulkit@gmail.com\"  # Your Gmail email address\n",
        "    receiver_email = \"vashishta.pulkit@gmail.com\"  # Receiver's email address\n",
        "    email_password = GMAIL_PASSWORD  # Your Gmail App Password (generated from Google account)\n",
        "\n",
        "    # Create the email content\n",
        "    subject = \"Meeting Summary and Conversation\"\n",
        "    body = f\"\"\"\n",
        "    <h2>Conversation:</h2>\n",
        "    <pre>{conversation}</pre>\n",
        "\n",
        "    <h2>Summary:</h2>\n",
        "    <pre>{summary}</pre>\n",
        "    \"\"\"\n",
        "\n",
        "    # Create MIME message\n",
        "    msg = MIMEMultipart()\n",
        "    msg['From'] = sender_email\n",
        "    msg['To'] = receiver_email\n",
        "    msg['Subject'] = subject\n",
        "\n",
        "    # Attach the body to the email\n",
        "    msg.attach(MIMEText(body, \"html\"))\n",
        "\n",
        "    # Connect to the server and send the email\n",
        "    try:\n",
        "        with smtplib.SMTP(smtp_server, smtp_port) as server:\n",
        "            server.starttls()  # Secure the connection\n",
        "            server.login(sender_email, email_password)  # Log in to your email account\n",
        "            server.sendmail(sender_email, receiver_email, msg.as_string())  # Send the email\n",
        "            print(\"Email sent successfully!\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "\n",
        "# Send the email\n",
        "send_email(conversation, summary)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QbILMa_a5bmX",
        "outputId": "5333cedf-8355-4d99-83e3-e32aa596384b",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Email sent successfully!\n"
          ]
        }
      ]
    }
  ]
}
